{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0ioj93KvNmNk6IaJ6t2wK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prem-cre/Multirag/blob/main/MultimodalAgents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7N01vRtuiK5"
      },
      "outputs": [],
      "source": [
        "# @title Install Required Libraries\n",
        "!pip install -qU langchain langgraph langchain_groq langchain_huggingface\n",
        "!pip install -qU faiss-cpu pypdf tiktoken tavily-python\n",
        "!pip install -qU langchain-community langchain-google-community\n",
        "!pip install -qU newspaper3k beautifulsoup4 requests\n",
        "!pip install -qU lxml[html_clean]\n",
        "\n",
        "# @title Core Imports and Configuration\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.agents import create_react_agent, AgentExecutor, Tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.tools import tool\n",
        "from tavily import TavilyClient\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure API Keys\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_api_key')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily')  # You'll need this\n",
        "\n",
        "# Initialize LLM and Embeddings\n",
        "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1)  # Changed to a supported model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Changed to a publicly available model\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Data Models for Evidence and Sources\n",
        "\n",
        "class SourceCredibility(Enum):\n",
        "    \"\"\"Credibility levels for sources\"\"\"\n",
        "    OFFICIAL = \"official\"  # Government, court documents\n",
        "    ACADEMIC = \"academic\"  # Peer-reviewed, scholarly\n",
        "    REPUTABLE = \"reputable\"  # Major news outlets, established organizations\n",
        "    GENERAL = \"general\"  # General websites, blogs\n",
        "    UNVERIFIED = \"unverified\"  # Unknown or questionable sources\n",
        "\n",
        "@dataclass\n",
        "class LegalSource:\n",
        "    \"\"\"Represents a legal source with metadata\"\"\"\n",
        "    url: str\n",
        "    title: str\n",
        "    content: str\n",
        "    credibility: SourceCredibility\n",
        "    # date_accessed: datetime\n",
        "    # date_published: Optional[str] = None\n",
        "    author: Optional[str] = None\n",
        "    jurisdiction: Optional[str] = None\n",
        "    citation: Optional[str] = None\n",
        "    hash: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Generate content hash for deduplication\n",
        "        self.hash = hashlib.md5(self.content.encode()).hexdigest()[:8]\n",
        "\n",
        "@dataclass\n",
        "class EvidenceItem:\n",
        "    \"\"\"Represents a piece of evidence in the legal research\"\"\"\n",
        "    claim: str\n",
        "    supporting_sources: List[LegalSource]\n",
        "    confidence_score: float  # 0-1\n",
        "    reasoning: str\n",
        "    contradictions: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    verification_status: str = \"pending\"\n",
        "\n",
        "@dataclass\n",
        "class LegalResearchResult:\n",
        "    \"\"\"Complete legal research result with chain of evidence\"\"\"\n",
        "    query: str\n",
        "    summary: str\n",
        "    evidence_chain: List[EvidenceItem]\n",
        "    legal_precedents: List[Dict[str, Any]]\n",
        "    jurisdictional_notes: Dict[str, str]\n",
        "    confidence_assessment: Dict[str, float]\n",
        "    timestamp: datetime = field(default_factory=datetime.now)"
      ],
      "metadata": {
        "id": "Uth_vk2CEZSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Implement Advanced Research Tools\n",
        "\n",
        "@tool\n",
        "def legal_document_search(query: str, jurisdiction: str = \"INDIAN union and state Federal\") -> str:\n",
        "    \"\"\"\n",
        "    Search legal documents, cases, and statutes.\n",
        "    Returns relevant legal information with citations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Search using Tavily with legal-focused query\n",
        "        legal_query = f\"legal {jurisdiction}jurisdiction {query}\"\n",
        "        results = tavily_client.search(\n",
        "            query=legal_query,\n",
        "            search_depth=\"advanced\",\n",
        "            max_results=5,\n",
        "            include_domains=[\"https://indiankanoon.org/\"],\n",
        "        )\n",
        "\n",
        "        formatted_results = []\n",
        "        for r in results.get('results', []):\n",
        "            formatted_results.append({\n",
        "                'title': r.get('title'),\n",
        "                'url': r.get('url'),\n",
        "                'content': r.get('content'),\n",
        "                'score': r.get('score', 0)\n",
        "            })\n",
        "\n",
        "        return json.dumps(formatted_results, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error searching legal documents: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def verify_legal_citation(citation: str) -> str:\n",
        "    \"\"\"\n",
        "    Verify if a legal citation is valid and retrieve its details.\n",
        "    Supports indian case law citations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        case_pattern = r'(\\d+)\\s+(\\w+\\.?\\s*\\d*[a-z]?)\\s+(\\d+)'\n",
        "        match = re.search(case_pattern, citation)\n",
        "\n",
        "        if match:\n",
        "            # Search for the actual case\n",
        "            search_results = tavily_client.search(\n",
        "                query=f'\"{citation}\" legal case',\n",
        "                max_results=5,\n",
        "                include_domains=[\"https://indiankanoon.org/\"],\n",
        "            )\n",
        "\n",
        "            if search_results.get('results'):\n",
        "                case_info = {\n",
        "                    'citation': citation,\n",
        "                    'verified': True,\n",
        "                    'sources': [r.get('url') for r in search_results['results']],\n",
        "                    'summary': search_results['results'][0].get('content', '')[:200]\n",
        "                }\n",
        "                return json.dumps(case_info, indent=2)\n",
        "\n",
        "        return json.dumps({'citation': citation, 'verified': False, 'error': 'Citation format not recognized'})\n",
        "    except Exception as e:\n",
        "        return f\"Error verifying citation: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def fact_check_legal_claim(claim: str, sources: List[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Fact-check a legal claim against multiple sources.\n",
        "    Returns verification status with supporting/contradicting evidence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Search for supporting and contradicting evidence\n",
        "        supporting_query = f'\"{claim}\" legal true accurate'\n",
        "        contradicting_query = f'\"{claim}\" legal false myth incorrect'\n",
        "\n",
        "        supporting = tavily_client.search(supporting_query, max_results=5)\n",
        "        contradicting = tavily_client.search(contradicting_query, max_results=3)\n",
        "\n",
        "        result = {\n",
        "            'claim': claim,\n",
        "            'supporting_evidence': [\n",
        "                {'source': r.get('url'), 'excerpt': r.get('content')[:150]}\n",
        "                for r in supporting.get('results', [])\n",
        "            ],\n",
        "            'contradicting_evidence': [\n",
        "                {'source': r.get('url'), 'excerpt': r.get('content')[:150]}\n",
        "                for r in contradicting.get('results', [])\n",
        "            ],\n",
        "            'confidence': 'high' if len(supporting.get('results', [])) > len(contradicting.get('results', [])) else 'low'\n",
        "        }\n",
        "\n",
        "        return json.dumps(result, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error fact-checking claim: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def extract_legal_precedents(case_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract legal precedents and cited cases from text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Common patterns for case citations\n",
        "        citation_patterns = [\n",
        "            r'\\d+\\s+U\\.S\\.\\s+\\d+',  # US Reports\n",
        "            r'\\d+\\s+S\\.Ct\\.\\s+\\d+',  # Supreme Court Reporter\n",
        "            r'\\d+\\s+F\\.\\d+\\s+\\d+',  # Federal Reporter\n",
        "            r'\\d+\\s+F\\.Supp\\.\\s+\\d+',  # Federal Supplement\n",
        "        ]\n",
        "\n",
        "        precedents = []\n",
        "        for pattern in citation_patterns:\n",
        "            matches = re.findall(pattern, case_text)\n",
        "            precedents.extend(matches)\n",
        "\n",
        "        # Remove duplicates and return\n",
        "        unique_precedents = list(set(precedents))\n",
        "        return json.dumps({\n",
        "            'precedents_found': len(unique_precedents),\n",
        "            'citations': unique_precedents[:10]  # Limit to 10 most relevant\n",
        "        }, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting precedents: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def analyze_source_credibility(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze the credibility of a source based on domain, content, and other factors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        domain_credibility = {\n",
        "            'law.cornell.edu': SourceCredibility.OFFICIAL,\n",
        "            'justia.com': SourceCredibility.OFFICIAL,\n",
        "            'courtlistener.com': SourceCredibility.OFFICIAL,\n",
        "            'indiankanoon.org': SourceCredibility.OFFICIAL,\n",
        "            # 'supremecourt.gov': SourceCredibility.OFFICIAL,\n",
        "            # 'scholar.google.com': SourceCredibility.ACADEMIC,\n",
        "            # 'harvard.edu': SourceCredibility.ACADEMIC,\n",
        "            # 'yale.edu': SourceCredibility.ACADEMIC,\n",
        "            # 'reuters.com': SourceCredibility.REPUTABLE,\n",
        "            'apnews.com': SourceCredibility.REPUTABLE,\n",
        "        }\n",
        "\n",
        "        # Extract domain\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(url).netloc.lower()\n",
        "\n",
        "        # Check known domains\n",
        "        for known_domain, cred in domain_credibility.items():\n",
        "            if known_domain in domain:\n",
        "                return json.dumps({\n",
        "                    'url': url,\n",
        "                    'domain': domain,\n",
        "                    'credibility': cred.value,\n",
        "                    'trusted': True\n",
        "                })\n",
        "\n",
        "        # Default assessment\n",
        "        return json.dumps({\n",
        "            'url': url,\n",
        "            'domain': domain,\n",
        "            'credibility': SourceCredibility.GENERAL.value,\n",
        "            'trusted': False,\n",
        "            'note': 'Unknown source - verify independently'\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return f\"Error analyzing source: {str(e)}\""
      ],
      "metadata": {
        "id": "wyRdVy3qPRoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the Advanced Legal Research Agent\n",
        "\n",
        "class LegalResearchAgent:\n",
        "    \"\"\"Advanced agent for legal research with multi-step reasoning\"\"\"\n",
        "\n",
        "    def __init__(self, llm, tools):\n",
        "        self.llm = llm\n",
        "        self.tools = tools\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            k=10\n",
        "        )\n",
        "\n",
        "        # Create specialized prompt for legal research\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert legal research assistant with deep knowledge of law and legal procedures.\n",
        "\n",
        "Your approach to legal research:\n",
        "1. **Understand the Query**: Identify the legal question, jurisdiction, and relevant areas of law\n",
        "2. **Gather Evidence**: Search for relevant cases, statutes, and legal documents\n",
        "3. **Verify Sources**: Check credibility and authenticity of all sources\n",
        "4. **Extract Precedents**: Identify relevant legal precedents and citations\n",
        "5. **Fact-Check Claims**: Verify all legal claims against multiple sources\n",
        "6. **Build Evidence Chain**: Create a logical chain of evidence supporting your conclusions\n",
        "7. **Assess Confidence**: Evaluate the strength of your findings\n",
        "\n",
        "For each research task:\n",
        "- Always cite specific cases and statutes\n",
        "- Verify all citations are real and accurate\n",
        "- Note jurisdictional limitations\n",
        "- Identify potential contradictions or conflicts\n",
        "- Provide confidence assessments for your conclusions\n",
        "\n",
        "Available tools:\n",
        "{tools}\n",
        "\n",
        "Use these tools to conduct thorough legal research before providing your final analysis.\"\"\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "        # Create the agent\n",
        "        self.agent = create_react_agent(\n",
        "            llm=self.llm,\n",
        "            tools=self.tools,\n",
        "            prompt=self.prompt\n",
        "        )\n",
        "\n",
        "        self.executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            memory=self.memory,\n",
        "            verbose=True,\n",
        "            max_iterations=15,\n",
        "            handle_parsing_errors=True,\n",
        "            return_intermediate_steps=True\n",
        "        )\n",
        "\n",
        "    def research(self, query: str) -> LegalResearchResult:\n",
        "        \"\"\"Conduct comprehensive legal research\"\"\"\n",
        "        try:\n",
        "            # Execute the research\n",
        "            result = self.executor.invoke({\"input\": query})\n",
        "\n",
        "            # Parse the results into structured format\n",
        "            return self._parse_research_results(query, result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in legal research: {str(e)}\")\n",
        "            return LegalResearchResult(\n",
        "                query=query,\n",
        "                summary=f\"Error conducting research: {str(e)}\",\n",
        "                evidence_chain=[],\n",
        "                legal_precedents=[],\n",
        "                jurisdictional_notes={},\n",
        "                confidence_assessment={\"overall\": 0.0}\n",
        "            )\n",
        "\n",
        "    def _parse_research_results(self, query: str, raw_result: Dict) -> LegalResearchResult:\n",
        "        \"\"\"Parse agent results into structured legal research result\"\"\"\n",
        "        # Extract key information from the agent's response\n",
        "        output = raw_result.get('output', '')\n",
        "        intermediate_steps = raw_result.get('intermediate_steps', [])\n",
        "\n",
        "        # Build evidence chain from intermediate steps\n",
        "        evidence_chain = []\n",
        "        legal_precedents = []\n",
        "\n",
        "        for action, observation in intermediate_steps:\n",
        "            if hasattr(action, 'tool'):\n",
        "                if action.tool == 'legal_document_search':\n",
        "                    # Parse search results\n",
        "                    try:\n",
        "                        results = json.loads(observation)\n",
        "                        for r in results:\n",
        "                            evidence_chain.append(EvidenceItem(\n",
        "                                claim=f\"Found relevant document: {r.get('title', 'Unknown')}\",\n",
        "                                supporting_sources=[LegalSource(\n",
        "                                    url=r.get('url', ''),\n",
        "                                    title=r.get('title', ''),\n",
        "                                    content=r.get('content', ''),\n",
        "                                    credibility=SourceCredibility.GENERAL,\n",
        "                                    date_accessed=datetime.now()\n",
        "                                )],\n",
        "                                confidence_score=r.get('score', 0.5),\n",
        "                                reasoning=\"Direct search result\"\n",
        "                            ))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                elif action.tool == 'extract_legal_precedents':\n",
        "                    # Parse precedents\n",
        "                    try:\n",
        "                        precedents_data = json.loads(observation)\n",
        "                        legal_precedents.extend(precedents_data.get('citations', []))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        # Create final result\n",
        "        return LegalResearchResult(\n",
        "            query=query,\n",
        "            summary=output,\n",
        "            evidence_chain=evidence_chain,\n",
        "            legal_precedents=[{'citation': p, 'verified': True} for p in legal_precedents],\n",
        "            jurisdictional_notes={'primary': 'US Federal', 'limitations': 'Results may vary by state'},\n",
        "            confidence_assessment={'overall': 0.8, 'source_quality': 0.9}\n",
        "        )"
      ],
      "metadata": {
        "id": "tq7z6cbsYp2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Implement Multi-Step Fact-Checking Process\n",
        "\n",
        "class LegalFactChecker:\n",
        "    \"\"\"Multi-step fact-checking system for legal claims\"\"\"\n",
        "\n",
        "    def __init__(self, llm, tools):\n",
        "        self.llm = llm\n",
        "        self.tools = tools\n",
        "        self.verification_steps = [\n",
        "            self._initial_claim_analysis,\n",
        "            self._source_verification,\n",
        "            self._cross_reference_check,\n",
        "            self._contradiction_analysis,\n",
        "            self._final_verification\n",
        "        ]\n",
        "\n",
        "    def _initial_claim_analysis(self, claim: str) -> Dict[str, Any]:\n",
        "        \"\"\"Step 1: Analyze the claim structure and identify key elements\"\"\"\n",
        "        prompt = f\"\"\"Analyze this legal claim and identify:\n",
        "        1. Main legal assertion\n",
        "        2. Jurisdiction mentioned\n",
        "        3. Legal concepts involved\n",
        "        4. Specific facts claimed\n",
        "\n",
        "        Claim: {claim}\n",
        "        \"\"\"\n",
        "\n",
        "        # Use LLM to analyze the claim\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return {\n",
        "            \"step\": \"initial_analysis\",\n",
        "            \"claim\": claim,\n",
        "            \"analysis\": response.content,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }\n",
        "\n",
        "    def _source_verification(self, current_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Step 2: Verify sources for the claim\"\"\"\n",
        "        claim = current_data.get('claim')\n",
        "        if not claim:\n",
        "            return {\"step\": \"source_verification\", \"error\": \"Claim not found in data\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        # Use the fact_check_legal_claim tool\n",
        "        # Pass the claim as a string, not the whole current_data dictionary\n",
        "        result_json_str = fact_check_legal_claim.invoke(claim)\n",
        "        try:\n",
        "            result = json.loads(result_json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"step\": \"source_verification\", \"error\": f\"Failed to parse fact_check_legal_claim output: {e}\", \"raw_output\": result_json_str, \"timestamp\": datetime.now()}\n",
        "\n",
        "        return {\n",
        "            \"step\": \"source_verification\",\n",
        "            \"sources_found\": result,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }\n",
        "\n",
        "    def _cross_reference_check(self, current_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Step 3: Cross-reference multiple sources\"\"\"\n",
        "        sources_data = current_data.get('sources_found')\n",
        "        if not sources_data:\n",
        "             return {\"step\": \"cross_reference\", \"error\": \"Sources data not found\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        credible_sources_list = []\n",
        "        supporting_evidence = sources_data.get('supporting_evidence', [])\n",
        "\n",
        "        for source_item in supporting_evidence:\n",
        "            source_url = source_item.get('source')\n",
        "            if source_url:\n",
        "                try:\n",
        "                    cred_check_json_str = analyze_source_credibility.invoke(source_url)\n",
        "                    cred_check = json.loads(cred_check_json_str)\n",
        "                    credible_sources_list.append(cred_check)\n",
        "                except json.JSONDecodeError as e:\n",
        "                     credible_sources_list.append({\"url\": source_url, \"credibility\": \"parse_error\", \"error\": str(e)})\n",
        "                except Exception as e:\n",
        "                     credible_sources_list.append({\"url\": source_url, \"credibility\": \"tool_error\", \"error\": str(e)})\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"step\": \"cross_reference\",\n",
        "            \"credible_sources\": credible_sources_list,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }\n",
        "\n",
        "    def _contradiction_analysis(self, current_data: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Step 4: Analyze contradictions\"\"\"\n",
        "        sources_data = current_data.get('sources_found')\n",
        "        if not sources_data:\n",
        "            return {\"step\": \"contradiction_analysis\", \"error\": \"Sources data not found for contradiction analysis\", \"timestamp\": datetime.now()}\n",
        "\n",
        "        contradictions = sources_data.get('contradicting_evidence', [])\n",
        "\n",
        "        return {\n",
        "            \"step\": \"contradiction_analysis\",\n",
        "            \"contradictions_found\": len(contradictions),\n",
        "            \"details\": contradictions,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }\n",
        "\n",
        "    def _final_verification(self, all_steps: List[Dict]) -> Dict[str, Any]:\n",
        "        \"\"\"Step 5: Final verification and confidence scoring\"\"\"\n",
        "        supporting_count = 0\n",
        "        contradiction_count = 0\n",
        "        errors_in_steps = 0\n",
        "        credible_sources_count = 0\n",
        "\n",
        "        for step_data in all_steps:\n",
        "            if step_data.get('error'):\n",
        "                errors_in_steps += 1\n",
        "            elif step_data.get('step') == 'source_verification':\n",
        "                sources_data = step_data.get('sources_found', {})\n",
        "                supporting_count += len(sources_data.get('supporting_evidence', []))\n",
        "                contradiction_count += len(sources_data.get('contradicting_evidence', []))\n",
        "            elif step_data.get('step') == 'cross_reference':\n",
        "                 credible_sources_count += len([cs for cs in step_data.get('credible_sources', []) if cs.get('trusted', False) or cs.get('credibility') in ['official', 'academic', 'reputable']])\n",
        "\n",
        "\n",
        "        # Calculate confidence\n",
        "        total_evidence = supporting_count + contradiction_count\n",
        "        if total_evidence == 0:\n",
        "             confidence = 0.5 if errors_in_steps == 0 else 0.0 # Neutral if no evidence but no errors, low if errors\n",
        "        else:\n",
        "            # Factor in credible sources count for a more nuanced confidence score\n",
        "            confidence = max(0, min(1, (supporting_count + credible_sources_count - contradiction_count) / (total_evidence + credible_sources_count)))\n",
        "\n",
        "        return {\n",
        "            \"step\": \"final_verification\",\n",
        "            \"confidence_score\": confidence,\n",
        "            \"verification_complete\": True,\n",
        "            \"timestamp\": datetime.now()\n",
        "        }\n",
        "\n",
        "\n",
        "    def verify_claim(self, claim: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute multi-step fact-checking process\"\"\"\n",
        "        results = []\n",
        "        current_data: Dict[str, Any] = {\"claim\": claim} # Initialize current_data with the claim\n",
        "\n",
        "        for step_func in self.verification_steps:\n",
        "            try:\n",
        "                # Pass current_data to each step function\n",
        "                step_result = step_func(current_data)\n",
        "                results.append(step_result)\n",
        "                # Update current_data with the results of the current step\n",
        "                current_data.update(step_result)\n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    \"step\": step_func.__name__,\n",
        "                    \"error\": str(e),\n",
        "                    \"timestamp\": datetime.now()\n",
        "                })\n",
        "                # If a step fails, we might want to stop or continue with partial data\n",
        "                # For now, let's continue to see subsequent step errors.\n",
        "                # Depending on the error, current_data might be incomplete for the next step.\n",
        "\n",
        "\n",
        "        # Pass the full list of step results to the final verification step\n",
        "        final_result = self._final_verification(results)\n",
        "        results.append(final_result) # Add the final verification step result\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"claim\": claim,\n",
        "            \"verification_steps\": results,\n",
        "            \"final_confidence\": final_result.get('confidence_score', 0),\n",
        "            \"timestamp\": datetime.now()\n",
        "        }"
      ],
      "metadata": {
        "id": "wTcEOx0TF06T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Collect all defined tools\n",
        "# Make sure these functions are available in the global scope from your previous code blocks\n",
        "# They were defined using the @tool decorator which automatically registers them as callables.\n",
        "all_tools = [\n",
        "    legal_document_search,\n",
        "    verify_legal_citation,\n",
        "    fact_check_legal_claim,\n",
        "    extract_legal_precedents,\n",
        "    analyze_source_credibility\n",
        "]\n",
        "\n",
        "# @title Instantiate and Run the Legal Fact-Checker\n",
        "\n",
        "print(\"Initializing Legal Fact-Checker...\")\n",
        "# Re-using the 'llm' and 'all_tools' defined in previous sections\n",
        "fact_checker = LegalFactChecker(llm=llm, tools=all_tools)\n",
        "print(\"Legal Fact-Checker initialized.\")\n",
        "\n",
        "# --- Define a sample legal claim to fact-check ---\n",
        "# Example 1: A claim that is likely true (or easily verifiable)\n",
        "claim_1 = \"In the United States, a contract requires an offer, acceptance, and consideration to be legally binding.\"\n",
        "# Example 2: A more nuanced or potentially disputable claim\n",
        "claim_2 = \"A non-compete clause in an employment contract is generally unenforceable in California if it restricts an employee's ability to practice a lawful profession, trade, or business.\"\n",
        "# Example 3: A claim that might have contradictions or require deeper analysis\n",
        "claim_3 = \"All verbal agreements for real estate sales are unenforceable in all U.S. states.\"\n",
        "\n",
        "print(f\"\\n--- Fact-Checking Claim: {claim_2} ---\")\n",
        "verification_result = fact_checker.verify_claim(claim_2)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"✅ FACT-CHECKING REPORT COMPLETE ✅\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n**Claim:** {verification_result['claim']}\\n\")\n",
        "print(f\"**Final Confidence:** {verification_result['final_confidence']:.2f}\\n\")\n",
        "print(\"-\" * 30 + \" Detailed Steps \" + \"-\" * 30)\n",
        "\n",
        "# Store cross-references and citations found during verification\n",
        "cross_references = []\n",
        "citations = []\n",
        "\n",
        "for step_data in verification_result['verification_steps']:\n",
        "    step_name = step_data.get('step', 'Unknown Step').replace('_', ' ').title()\n",
        "    timestamp = step_data.get('timestamp', datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    print(f\"\\n### {step_name} (at {timestamp})\")\n",
        "\n",
        "    if step_data.get('error'):\n",
        "        print(f\"  ❌ Error: {step_data['error']}\")\n",
        "    elif step_data.get('analysis'):\n",
        "        print(f\"  **Analysis:**\\n{step_data['analysis']}\")\n",
        "    elif step_data.get('sources_found'):\n",
        "        sources = step_data['sources_found']\n",
        "        print(f\"  **Verification Status:** {sources.get('confidence', 'N/A')}\")\n",
        "        print(\"  **Supporting Evidence:**\")\n",
        "        for s in sources.get('supporting_evidence', []):\n",
        "            print(f\"    - URL: {s.get('source')} (Excerpt: {s.get('excerpt')})\")\n",
        "            cross_references.append(s.get('source'))\n",
        "        print(\"  **Contradicting Evidence:**\")\n",
        "        for c in sources.get('contradicting_evidence', []):\n",
        "            print(f\"    - URL: {c.get('source')} (Excerpt: {c.get('excerpt')})\")\n",
        "            cross_references.append(c.get('source'))\n",
        "    elif step_data.get('credible_sources'):\n",
        "        print(\"  **Cross-Referenced Credible Sources:**\")\n",
        "        for cs in step_data['credible_sources']:\n",
        "            print(f\"    - URL: {cs.get('url')} (Credibility: {cs.get('credibility')})\")\n",
        "            cross_references.append(cs.get('url'))\n",
        "    elif step_data.get('contradictions_found') is not None:\n",
        "        print(f\"  **Contradictions Found:** {step_data['contradictions_found']}\")\n",
        "        for d in step_data.get('details', []):\n",
        "            print(f\"    - URL: {d.get('source')} (Excerpt: {d.get('excerpt')})\")\n",
        "            cross_references.append(d.get('source'))\n",
        "    elif step_data.get('verification_complete'):\n",
        "        print(f\"  **Confidence Score:** {step_data['confidence_score']:.2f}\")\n",
        "        print(\"  Verification process concluded.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Display Cross-References and Citations at the end\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📚 Cross-References and Citations 📚\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if cross_references:\n",
        "    print(\"\\n**Cross-References (URLs):**\")\n",
        "    # Remove duplicates and print\n",
        "    for url in sorted(list(set(cross_references))):\n",
        "        print(f\"- {url}\")\n",
        "else:\n",
        "    print(\"\\nNo significant cross-references found during verification.\")\n",
        "\n",
        "if citations:\n",
        "    print(\"\\n**Citations:**\")\n",
        "    # Remove duplicates and print\n",
        "    for citation in sorted(list(set(citations))):\n",
        "        print(f\"- {citation}\")\n",
        "else:\n",
        "    print(\"\\nNo specific legal citations found during verification steps.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Optional: You can run another claim\n",
        "# print(f\"\\n--- Fact-Checking Claim: {claim_3} ---\")\n",
        "# verification_result_2 = fact_checker.verify_claim(claim_3)\n",
        "# # ... (display verification_result_2 in a similar format)"
      ],
      "metadata": {
        "id": "r_QX0KA7GzGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}