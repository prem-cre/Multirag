{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQMYCh0JctoJsRpV0mtXlL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prem-cre/Multirag/blob/main/MultimodalAgents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Required Libraries\n",
        "!pip install -qU langchain langchain_groq langchain_huggingface\n",
        "!pip install -qU langchain-community langchain-google-community\n",
        "!pip install -qU tavily-python wikipedia-api beautifulsoup4 requests\n",
        "!pip install -qU tiktoken\n",
        "!pip install -qU lxml[html_clean]\n",
        "!pip install -qU faiss-cpu pypdf tiktoken tavily-python\n",
        "!pip install -qU wikipedia # Added wikipedia package\n",
        "!pip install langchain-google-genai\n"
      ],
      "metadata": {
        "id": "QCFCOahJhYt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7N01vRtuiK5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cell 2: Core Imports and Configuration\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from dataclasses import field\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from tavily import TavilyClient\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "# Configure API Keys\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_api_key')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = userdata.get('GOOGLE_CSE_ID')\n",
        "\n",
        "# Initialize services\n",
        "# llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1)\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# Initialize Gemini service (hypothetical example)\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # 👈 switch from \"gemini-pro\"\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "\n",
        "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "google_search_wrapper = GoogleSearchAPIWrapper(k=7)\n",
        "wikipedia_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=1000)\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia_wrapper)\n",
        "\n",
        "print(\"✅ All services initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Data Models for Legal Research\n",
        "\n",
        "class SourceCredibility(Enum):\n",
        "    \"\"\"Credibility levels for sources\"\"\"\n",
        "    OFFICIAL = \"official\"\n",
        "    ACADEMIC = \"academic\"\n",
        "    REPUTABLE = \"reputable\"\n",
        "    GENERAL = \"general\"\n",
        "    UNVERIFIED = \"unverified\"\n",
        "\n",
        "@dataclass\n",
        "class LegalSource:\n",
        "    \"\"\"Represents a legal source with metadata\"\"\"\n",
        "    url: str\n",
        "    title: str\n",
        "    content: str\n",
        "    credibility: SourceCredibility\n",
        "    # date_accessed: datetime = field(default_factory=datetime.now)\n",
        "    # date_published: Optional[str] = None\n",
        "    author: Optional[str] = None\n",
        "    jurisdiction: Optional[str] = None\n",
        "    citation: Optional[str] = None\n",
        "    hash: Optional[str] = None\n",
        "    relevance_score: float = 0.0\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.hash = hashlib.md5(self.content.encode()).hexdigest()[:8]\n",
        "\n",
        "@dataclass\n",
        "class EvidenceItem:\n",
        "    \"\"\"Represents a piece of evidence in the legal research\"\"\"\n",
        "    claim: str\n",
        "    supporting_sources: List[LegalSource]\n",
        "    confidence_score: float\n",
        "    reasoning: str\n",
        "    contradictions: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    verification_status: str = \"pending\"\n",
        "    legal_basis: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class LegalResearchResult:\n",
        "    \"\"\"Complete legal research result with chain of evidence\"\"\"\n",
        "    query: str\n",
        "    summary: str\n",
        "    evidence_chain: List[EvidenceItem]\n",
        "    legal_precedents: List[Dict[str, Any]]\n",
        "    jurisdictional_notes: Dict[str, str]\n",
        "    confidence_assessment: Dict[str, float]\n",
        "    citations: List[str]\n",
        "    # timestamp: datetime = field(default_factory=datetime.now)"
      ],
      "metadata": {
        "id": "Uth_vk2CEZSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Define Enhanced Research Tools\n",
        "\n",
        "@tool\n",
        "def legal_document_search(query: str, jurisdiction: str = \"Indian\") -> str:\n",
        "    \"\"\"\n",
        "    Search legal documents, cases, and statutes with enhanced Indian law focus.\n",
        "    Returns relevant legal information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Enhanced search query for Indian legal context\n",
        "        legal_query = f\"{jurisdiction} law legal {query} case judgment statute act\"\n",
        "\n",
        "        results = tavily_client.search(\n",
        "            query=legal_query,\n",
        "            search_depth=\"advanced\",\n",
        "            max_results=8,\n",
        "            include_domains=[\"indiankanoon.org\", \"scconline.com\", \"lawmin.gov.in\", \"legislative.gov.in\"],\n",
        "        )\n",
        "\n",
        "        # Return the raw results directly without formatting or filtering citations\n",
        "        return json.dumps(results.get('results', []), indent=2)\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": f\"Error searching legal documents: {str(e)}\"})\n",
        "\n",
        "@tool\n",
        "def google_legal_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Calls Google Search API for specialized legal search focusing on Indian legal databases.\n",
        "    Searches specifically in indiankanoon.org and scconline.com domains.\n",
        "    Returns relevant legal information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add site restrictions for Indian legal databases\n",
        "        search_query = f\"{query} site:indiankanoon.org OR site:scconline.com\"\n",
        "        search_results = google_search_wrapper.results(search_query, num_results=10)\n",
        "\n",
        "        # Return the raw results directly\n",
        "        return json.dumps(search_results, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error calling Google Search: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def wikipedia_legal_concepts(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches Wikipedia for legal concepts, landmark cases, and constitutional matters.\n",
        "    Provides background information and case summaries.\n",
        "    Returns the raw content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        wiki_result = wikipedia_tool.invoke(query)\n",
        "\n",
        "        # Return the raw wiki result\n",
        "        return wiki_result\n",
        "    except Exception as e:\n",
        "        return f\"Error calling Wikipedia: {str(e)}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "@tool\n",
        "def fact_check_legal_claim(claim: str) -> str:\n",
        "    \"\"\"\n",
        "    Comprehensive fact-checking of legal claims with Indian law focus.\n",
        "    Returns detailed verification data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Enhanced queries for Indian legal context\n",
        "        supporting_query = f'\"{claim}\" Indian law legal valid true correct Supreme Court High Court'\n",
        "        contradicting_query = f'\"{claim}\" Indian law legal invalid false incorrect exception limitation'\n",
        "\n",
        "        supporting = tavily_client.search(supporting_query, max_results=6)\n",
        "        contradicting = tavily_client.search(contradicting_query, max_results=4)\n",
        "\n",
        "        # Return the raw search results for supporting and contradicting evidence\n",
        "        result = {\n",
        "            'claim': claim,\n",
        "            'supporting_evidence_raw': supporting.get('results', []),\n",
        "            'contradicting_evidence_raw': contradicting.get('results', []),\n",
        "            'verification_summary': f\"Found {len(supporting.get('results', []))} potential supporting and {len(contradicting.get('results', []))} potential contradicting sources\"\n",
        "        }\n",
        "\n",
        "        return json.dumps(result, indent=2)\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": f\"Error fact-checking claim: {str(e)}\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "wyRdVy3qPRoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5491448"
      },
      "source": [
        "# @title Test Tool Output (Raw) for all Tools\n",
        "\n",
        "# Define test queries relevant to each tool\n",
        "test_queries = {\n",
        "    \"legal_document_search\": \"recent Supreme Court judgment on fundamental rights\",\n",
        "    \"google_legal_search\": \"Maharashtra Rent Control Act 1999\",\n",
        "    \"wikipedia_legal_concepts\": \"Doctrine of Basic Structure Indian Constitution\",\n",
        "    \"verify_legal_citation\": \"AIR 2020 SC 123\", # Example valid citation\n",
        "    \"fact_check_legal_claim\": \"Section 498A IPC is non-bailable\",\n",
        "    # Removed test for extract_legal_precedents\n",
        "}\n",
        "\n",
        "print(\"--- Testing Individual Tool Outputs ---\")\n",
        "print(\"Note: Examining raw output before LLM processing.\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test legal_document_search tool\n",
        "print(\"\\nTesting legal_document_search...\")\n",
        "query = test_queries[\"legal_document_search\"]\n",
        "print(f\"Query: {query}\")\n",
        "search_output = legal_document_search.invoke(query)\n",
        "print(\"Raw Output:\")\n",
        "print(search_output[:100] + \"...\" if len(search_output) > 1000 else search_output) # Print truncated output for brevity\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Test google_legal_search tool\n",
        "print(\"\\nTesting google_legal_search...\")\n",
        "query = test_queries[\"google_legal_search\"]\n",
        "print(f\"Query: {query}\")\n",
        "google_output = google_legal_search.invoke(query)\n",
        "print(\"Raw Output:\")\n",
        "print(google_output[:100] + \"...\" if len(google_output) > 1000 else google_output)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Test wikipedia_legal_concepts tool\n",
        "print(\"\\nTesting wikipedia_legal_concepts...\")\n",
        "query = test_queries[\"wikipedia_legal_concepts\"]\n",
        "print(f\"Query: {query}\")\n",
        "wiki_output = wikipedia_legal_concepts.invoke(query)\n",
        "print(\"Raw Output:\")\n",
        "print(wiki_output[:1000] + \"...\" if len(wiki_output) > 1000 else wiki_output)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "\n",
        "# Test fact_check_legal_claim tool\n",
        "print(\"\\nTesting fact_check_legal_claim...\")\n",
        "claim = test_queries[\"fact_check_legal_claim\"]\n",
        "print(f\"Claim: {claim}\")\n",
        "fact_check_output = fact_check_legal_claim.invoke(claim)\n",
        "print(\"Raw Output:\")\n",
        "print(fact_check_output[:100] + \"...\" if len(fact_check_output) > 1000 else fact_check_output)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "print(\"\\n--- Individual Tool Testing Complete ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Legal Research Agent with Structured Output (Fixed)\n",
        "\n",
        "class EnhancedLegalResearchAgent:\n",
        "    \"\"\"Advanced legal research agent with structured analysis and strong prompting\"\"\"\n",
        "\n",
        "    def __init__(self, llm, tools):\n",
        "        self.llm = llm\n",
        "        self.tools = tools\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            return_messages=True,\n",
        "            k=10\n",
        "        )\n",
        "\n",
        "        # Get tool names for the prompt\n",
        "        tool_names = [tool.name for tool in tools]\n",
        "        tool_descriptions = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
        "\n",
        "        # Enhanced comprehensive prompt with structured output requirements\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert Indian legal research assistant with comprehensive knowledge of Indian law, statutes, and legal procedures. You provide thorough, well-structured legal analysis.\n",
        "\n",
        "## YOUR APPROACH TO LEGAL RESEARCH:\n",
        "\n",
        "1. **Query Analysis**: Identify legal issues, applicable laws, jurisdiction, and key legal concepts\n",
        "2. **Comprehensive Research**: Search relevant cases, statutes, acts, and legal documents\n",
        "3. **Source Verification**: Verify credibility and authenticity of all sources\n",
        "4. **Precedent Analysis**: Extract and analyze relevant legal precedents and landmark cases\n",
        "5. **Fact Verification**: Cross-check all legal claims against multiple authoritative sources\n",
        "6. **Evidence Synthesis**: Build a logical chain of evidence with proper legal reasoning\n",
        "7. **Confidence Assessment**: Evaluate the strength and reliability of findings\n",
        "\n",
        "## AVAILABLE TOOLS:\n",
        "{tool_descriptions}\n",
        "\n",
        "You have access to the following tools: {tool_names}\n",
        "\n",
        "## STRUCTURED OUTPUT FORMAT:\n",
        "\n",
        "Your final analysis MUST be structured with these EXACT headings:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 6. CITATIONS AND REFERENCES\n",
        "- Minimum 5-6 authoritative citations\n",
        "- Format: Case name, Citation, Court, Year\n",
        "- Include statutory references\n",
        "- Academic sources (if used)\n",
        "\n",
        "\n",
        "\n",
        "Use the tools systematically to gather comprehensive information before providing your structured analysis.\"\"\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "        ])\n",
        "\n",
        "        # Partial the prompt with tool information\n",
        "        self.prompt = self.prompt.partial(\n",
        "            tool_names=\", \".join(tool_names),\n",
        "            tool_descriptions=tool_descriptions,\n",
        "            tools=tool_descriptions  # For backward compatibility\n",
        "        )\n",
        "\n",
        "        # Create the agent\n",
        "        # Ensure handle_parsing_errors is set to True and return_intermediate_steps is True\n",
        "        self.agent = create_react_agent(\n",
        "            llm=self.llm,\n",
        "            tools=self.tools,\n",
        "            prompt=self.prompt\n",
        "        )\n",
        "\n",
        "        self.executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            memory=self.memory,\n",
        "            verbose=True,\n",
        "            max_iterations=20,\n",
        "            handle_parsing_errors=True, # Keep this\n",
        "            return_intermediate_steps=True # Keep this\n",
        "        )\n",
        "\n",
        "    def research(self, query: str) -> LegalResearchResult:\n",
        "        \"\"\"Conduct comprehensive legal research\"\"\"\n",
        "        try:\n",
        "            # Execute the research\n",
        "            # Pass input as a dictionary\n",
        "            result = self.executor.invoke({\"input\": query})\n",
        "\n",
        "            # Parse and structure the results\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Error in legal research: {str(e)}\")\n",
        "            return LegalResearchResult(\n",
        "                query=query,\n",
        "                summary=f\"Error conducting research: {str(e)}\",\n",
        "                evidence_chain=[],\n",
        "                legal_precedents=[],\n",
        "                jurisdictional_notes={},\n",
        "                confidence_assessment={\"overall\": 0.0},\n",
        "                citations=[]\n",
        "            )\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "tq7z6cbsYp2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Multi-Step Fact-Checking Process with LangChain Chains\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "class EnhancedLegalFactChecker:\n",
        "    \"\"\"Enhanced fact-checking system using LangChain chains\"\"\"\n",
        "\n",
        "    def __init__(self, llm, tools):\n",
        "        self.llm = llm\n",
        "        self.tools = tools\n",
        "\n",
        "        # Create specialized chains for different steps\n",
        "        self._create_analysis_chains()\n",
        "\n",
        "    def _create_analysis_chains(self):\n",
        "        \"\"\"Create LangChain chains for structured analysis\"\"\"\n",
        "\n",
        "        # Chain for initial claim analysis\n",
        "        self.claim_analysis_chain = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=[\"claim\"],\n",
        "                template=\"\"\"Analyze this legal claim in detail:\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Provide a structured analysis with:\n",
        "1. **Main Legal Assertion**: What is the core legal claim?\n",
        "2. **Jurisdiction**: Which legal system/jurisdiction applies?\n",
        "3. **Legal Concepts**: What legal principles are involved?\n",
        "4. **Factual Elements**: What specific facts are claimed?\n",
        "5. **Potential Issues**: Any ambiguities or concerns?\n",
        "\n",
        "Format your response as a detailed legal analysis.\"\"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Chain for synthesizing evidence\n",
        "        self.evidence_synthesis_chain = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=[\"evidence\", \"claim\"],\n",
        "                template=\"\"\"Synthesize the following evidence for the legal claim:\n",
        "\n",
        "Claim: {claim}\n",
        "\n",
        "Evidence Found:\n",
        "{evidence}\n",
        "\n",
        "Provide:\n",
        "1. **Strength of Evidence**: How strong is the supporting evidence?\n",
        "2. **Contradictions**: Any conflicting information?\n",
        "3. **Gaps**: What information is missing?\n",
        "4. **Overall Assessment**: Your professional legal opinion\n",
        "\n",
        "Be thorough and cite specific sources.\"\"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Chain for final verification report\n",
        "        self.final_report_chain = LLMChain(\n",
        "            llm=self.llm,\n",
        "            prompt=PromptTemplate(\n",
        "                input_variables=[\"claim\", \"analysis\", \"evidence\", \"confidence\"],\n",
        "                template=\"\"\"Generate a comprehensive legal fact-checking report:\n",
        "\n",
        "**CLAIM UNDER REVIEW**: {claim}\n",
        "\n",
        "**INITIAL ANALYSIS**: {analysis}\n",
        "\n",
        "**EVIDENCE SUMMARY**: {evidence}\n",
        "\n",
        "**CONFIDENCE LEVEL**: {confidence}\n",
        "\n",
        "Structure your report with these sections:\n",
        "\n",
        "## 1. EXECUTIVE SUMMARY\n",
        "- Brief overview of findings\n",
        "- Verification status (Verified/Partially Verified/Unverified/False)\n",
        "\n",
        "## 2. DETAILED LEGAL ANALYSIS\n",
        "- Applicable laws and statutes\n",
        "- Relevant case law\n",
        "- Legal principles involved\n",
        "\n",
        "## 3. EVIDENCE EVALUATION\n",
        "- Supporting evidence strength\n",
        "- Contradicting evidence analysis\n",
        "- Source credibility assessment\n",
        "\n",
        "## 4. LEGAL CITATIONS\n",
        "- List all relevant citations found\n",
        "- Include case names, citations, and years **IMPORTANT MANDATORY**\n",
        "\n",
        "## 5. CONCLUSION AND CONFIDENCE ASSESSMENT\n",
        "- Final determination\n",
        "- Confidence percentage with reasoning\n",
        "- Recommendations for further verification if needed\n",
        "\n",
        "Ensure all citations follow proper legal citation format.\"\"\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def verify_claim(self, claim: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute enhanced multi-step fact-checking process\"\"\"\n",
        "        results = {\n",
        "            \"claim\": claim,\n",
        "            \"timestamp\": datetime.now(),\n",
        "            \"steps\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Initial Claim Analysis using LangChain\n",
        "            print(\"Step 1: Analyzing claim structure...\")\n",
        "            claim_analysis = self.claim_analysis_chain.run(claim=claim)\n",
        "            results[\"steps\"].append({\n",
        "                \"step\": \"claim_analysis\",\n",
        "                \"output\": claim_analysis,\n",
        "                \"timestamp\": datetime.now()\n",
        "            })\n",
        "\n",
        "            # Step 2: Fact-check the claim using tool\n",
        "            print(\"Step 2: Fact-checking claim...\")\n",
        "            fact_check_result = fact_check_legal_claim.invoke(claim)\n",
        "            fact_check_data = json.loads(fact_check_result)\n",
        "            print(fact_check_data)\n",
        "            results[\"steps\"].append({\n",
        "                \"step\": \"fact_checking\",\n",
        "                \"output\": fact_check_data,\n",
        "                \"timestamp\": datetime.now()\n",
        "            })\n",
        "\n",
        "            # Step 3: Verify sources credibility\n",
        "            print(\"Step 3: Verifying source credibility...\")\n",
        "            credible_sources = []\n",
        "            all_sources = (\n",
        "                fact_check_data.get('supporting_evidence', []) +\n",
        "                fact_check_data.get('contradicting_evidence', [])\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "            results[\"steps\"].append({\n",
        "                \"step\": \"source_verification\",\n",
        "                \"output\": all_sources,\n",
        "                \"timestamp\": datetime.now()\n",
        "            })\n",
        "\n",
        "            # Step 4: Search for legal precedents\n",
        "            print(\"Step 4: Searching for legal precedents...\")\n",
        "            precedent_search = legal_document_search.invoke(claim, \"Indian\")\n",
        "            precedent_data = json.loads(precedent_search)\n",
        "            results[\"steps\"].append({\n",
        "                \"step\": \"precedent_search\",\n",
        "                \"output\": precedent_data,\n",
        "                \"timestamp\": datetime.now()\n",
        "            })\n",
        "\n",
        "            # Step 5: Synthesize evidence\n",
        "            print(\"Step 5: Synthesizing evidence...\")\n",
        "            evidence_summary = json.dumps({\n",
        "                \"supporting\": len(fact_check_data.get('supporting_evidence', [])),\n",
        "                \"contradicting\": len(fact_check_data.get('contradicting_evidence', [])),\n",
        "                \"credible_sources\": len([s for s in credible_sources if s.get('trusted', False)]),\n",
        "                \"precedents_found\": len(precedent_data) if isinstance(precedent_data, list) else 0\n",
        "            })\n",
        "\n",
        "            synthesis = self.evidence_synthesis_chain.run(\n",
        "                evidence=evidence_summary,\n",
        "                claim=claim\n",
        "            )\n",
        "            results[\"steps\"].append({\n",
        "                \"step\": \"evidence_synthesis\",\n",
        "                \"output\": synthesis,\n",
        "                \"timestamp\": datetime.now()\n",
        "            })\n",
        "\n",
        "\n",
        "\n",
        "            # Step 7: Generate final report\n",
        "            print(\"Step 6: Generating final report...\")\n",
        "            final_report = self.final_report_chain.run(\n",
        "                claim=claim,\n",
        "                analysis=claim_analysis,\n",
        "                evidence=synthesis\n",
        "            )\n",
        "\n",
        "            results[\"final_report\"] = final_report\n",
        "            results[\"verification_complete\"] = True\n",
        "\n",
        "        except Exception as e:\n",
        "            results[\"error\"] = str(e)\n",
        "            results[\"verification_complete\"] = False\n",
        "\n",
        "\n",
        "        return results\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "wTcEOx0TF06T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Summary Chain for Final Output\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "def create_summary_chain(llm):\n",
        "    \"\"\"Create a summarization chain using LangChain's built-in functionality\"\"\"\n",
        "    return load_summarize_chain(\n",
        "        llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        return_intermediate_steps=True,\n",
        "        map_prompt=PromptTemplate(\n",
        "            template=\"\"\"Summarize the following legal information:\n",
        "{text}\n",
        "\n",
        "Focus on:\n",
        "- Key legal points\n",
        "- Important citations\n",
        "- Relevant precedents\"\"\",\n",
        "            input_variables=[\"text\"]\n",
        "        ),\n",
        "        combine_prompt=PromptTemplate(\n",
        "            template=\"\"\"Combine these legal summaries into a comprehensive overview:\n",
        "{text}\n",
        "\n",
        "Provide:\n",
        "1. Main legal findings\n",
        "2. Critical citations\n",
        "3. Overall conclusion\"\"\",\n",
        "            input_variables=[\"text\"]\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "tA4meJqnHwfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tools = [\n",
        "    legal_document_search,\n",
        "    google_legal_search,\n",
        "    fact_check_legal_claim,\n",
        "    wikipedia_legal_concepts\n",
        "\n",
        "]\n",
        "\n",
        "# @title Instantiate and Run the Legal Fact-Checker and Research Agent\n",
        "\n",
        "print(\"Initializing Legal Fact-Checker and Research Agent...\")\n",
        "# Re-using the 'llm' and 'all_tools' defined in previous sections\n",
        "fact_checker = EnhancedLegalFactChecker(llm=llm, tools=all_tools)\n",
        "legal_research_agent = EnhancedLegalResearchAgent(llm=llm, tools=all_tools) # Also removed from agent tools\n",
        "print(\"Legal Fact-Checker and Research Agent initialized.\")\n",
        "\n",
        "# --- Define a sample legal claim to fact-check and research ---\n",
        "# Example 1: A claim that is likely true (or easily verifiable)\n",
        "claim_1 = \"In the United States, a contract requires an offer, acceptance, and consideration to be legally binding.\"\n",
        "# Example 2: A more nuanced or potentially disputable claim\n",
        "claim_2 = \"A non-compete clause in an employment contract is generally unenforceable in California if it restricts an employee's ability to practice a lawful profession, trade, or business.\"\n",
        "# Example 3: A claim that might have contradictions or require deeper analysis\n",
        "claim_3 = \"All verbal agreements for real estate sales are unenforceable in all U.S. states.\"\n",
        "# Example 4: Indian Law Claim (matches previous example)\n",
        "claim_4 = \"The Supreme Court of India in Kesavananda Bharati case established that the basic structure of the Constitution cannot be amended by Parliament.\"\n",
        "\n",
        "\n",
        "# @title Execute Comprehensive Legal Research (using the agent)\n",
        "def execute_legal_research(claim, agent, fact_checker):\n",
        "    \"\"\"Execute comprehensive legal research with structured output\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"🔍 COMPREHENSIVE LEGAL RESEARCH REPORT\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"\\n📋 **CLAIM UNDER EXAMINATION:**\\n{claim}\")\n",
        "    print(\"\\n\" + \"-\"*100)\n",
        "\n",
        "    # Step 1: Fact-checking\n",
        "    print(\"\\n⚖️ **EXECUTING MULTI-STEP FACT-CHECKING PROCESS...**\\n\")\n",
        "    # Use the fact_checker directly for the fact-checking steps\n",
        "    fact_check_result = fact_checker.verify_claim(claim)\n",
        "\n",
        "    # Display fact-checking steps\n",
        "    if fact_check_result.get(\"verification_steps\"):\n",
        "        for i, step in enumerate(fact_check_result[\"verification_steps\"], 1):\n",
        "            step_name = step.get('step', 'Unknown Step').replace(\"_\", \" \").title()\n",
        "            print(f\"\\n📌 Step {i}: {step_name}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            if step.get(\"error\"):\n",
        "                 print(f\"  ❌ Error: {step['error']}\")\n",
        "            elif step.get(\"analysis\"):\n",
        "                # Corrected f-string syntax\n",
        "                analysis_text = step['analysis']\n",
        "                print(f\"  **Analysis:**\\n{analysis_text[:500]}...\" if len(analysis_text) > 500 else analysis_text)\n",
        "            elif step.get(\"sources_found\"):\n",
        "                 data = step[\"sources_found\"]\n",
        "                 print(f\"✓ Supporting Evidence: {len(data.get('supporting_evidence_raw', []))} sources\")\n",
        "                 print(f\"✗ Contradicting Evidence: {len(data.get('contradicting_evidence_raw', []))} sources\")\n",
        "                 print(f\"📊 Initial Confidence: {data.get('verification_summary', 'N/A')}\") # Display summary from tool output\n",
        "            elif step.get(\"credible_sources\"):\n",
        "                 credible = len([s for s in step[\"credible_sources\"] if s.get('trusted', False) or s.get('credibility') in ['official', 'academic', 'reputable']])\n",
        "                 print(f\"🔐 Credible Sources Verified: {credible}/{len(step['credible_sources'])}\")\n",
        "            elif step.get(\"contradictions_found\") is not None:\n",
        "                 print(f\"  **Contradictions Found:** {step['contradictions_found']}\")\n",
        "            elif step.get(\"confidence_score\") is not None:\n",
        "                 print(f\"  **Confidence Score for Step:** {step['confidence_score']:.2%}\")\n",
        "\n",
        "\n",
        "    # Display final confidence score from fact checker\n",
        "    print(f\"\\n\\n🎯 **FINAL FACT-CHECKING CONFIDENCE SCORE: {fact_check_result.get('final_confidence', 0):.2%}**\")\n",
        "\n",
        "    # Step 2: Legal Research (using the agent)\n",
        "    print(\"\\n\\n\" + \"-\"*100)\n",
        "    print(\"📚 **CONDUCTING IN-DEPTH LEGAL RESEARCH (using Agent)...**\\n\")\n",
        "\n",
        "    # The agent will use its tools to perform the research based on the query\n",
        "    research_result = agent.research(claim)\n",
        "\n",
        "    # Display structured legal research output from the agent's final answer\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"📑 **STRUCTURED LEGAL ANALYSIS (from Agent)**\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # Parse and display the structured output from the agent's summary\n",
        "    output_text = research_result.summary\n",
        "\n",
        "    # Extract sections using regex or string parsing (adjust patterns if needed based on agent output)\n",
        "    sections = {\n",
        "        \"1. LEGAL ISSUE IDENTIFICATION\": r\"### 1\\. LEGAL ISSUE IDENTIFICATION(.*?)(?=###|$)\",\n",
        "        \"2. APPLICABLE LAWS AND STATUTES\": r\"### 2\\. APPLICABLE LAWS AND STATUTES(.*?)(?=###|$)\",\n",
        "        \"3. JUDICIAL PRECEDENTS AND CASE LAW\": r\"### 3\\. JUDICIAL PRECEDENTS AND CASE LAW(.*?)(?=###|$)\",\n",
        "        \"4. LEGAL ANALYSIS AND INTERPRETATION\": r\"### 4\\. LEGAL ANALYSIS AND INTERPRETATION(.*?)(?=###|$)\",\n",
        "        \"5. CONCLUSIONS AND RECOMMENDATIONS\": r\"### 5\\. CONCLUSIONS AND RECOMMENDATIONS(.*?)(?=###|$)\",\n",
        "        \"6. CITATIONS AND REFERENCES\": r\"### 6\\. CITATIONS AND REFERENCES(.*?)(?=###|$)\"\n",
        "    }\n",
        "\n",
        "    for section_title, pattern in sections.items():\n",
        "        match = re.search(pattern, output_text, re.DOTALL | re.IGNORECASE)\n",
        "        if match:\n",
        "            print(f\"\\n### {section_title}\")\n",
        "            print(\"-\" * 50)\n",
        "            content = match.group(1).strip()\n",
        "            print(content if content else \"No specific information found for this section.\")\n",
        "        else:\n",
        "            # If structured format not found, display the relevant part of the output\n",
        "            print(f\"\\n### {section_title}\")\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"Could not find section '{section_title}' in agent output. Full output snippet:\")\n",
        "            # Find a relevant snippet around where the section *should* be\n",
        "            snippet_start = output_text.find(section_title) - 100\n",
        "            snippet_end = output_text.find(section_title) + 200\n",
        "            print(output_text[max(0, snippet_start):min(len(output_text), snippet_end)] + \"...\")\n",
        "\n",
        "\n",
        "    # Display citations and evidence sources collected by the agent\n",
        "    print(\"\\n\\n\" + \"=\"*100)\n",
        "    print(\"📖 **LEGAL CITATIONS AND EVIDENCE SOURCES (from Agent)**\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    if research_result.citations:\n",
        "        print(\"\\n**Agent-Extracted Citations:**\")\n",
        "        for i, citation in enumerate(research_result.citations[:10], 1):\n",
        "            print(f\"{i}. {citation}\")\n",
        "    else:\n",
        "        print(\"\\nNo specific citations extracted by the agent.\")\n",
        "\n",
        "    if research_result.evidence_chain:\n",
        "        print(\"\\n**Evidence Sources Used by Agent (Sample):**\")\n",
        "        for i, evidence in enumerate(research_result.evidence_chain[:5], 1):\n",
        "            print(f\"\\n{i}. Claim: {evidence.claim}\")\n",
        "            if evidence.supporting_sources:\n",
        "                source = evidence.supporting_sources[0]\n",
        "                print(f\"   Source URL: {source.url}\")\n",
        "                print(f\"   Credibility: {source.credibility.value}\")\n",
        "                print(f\"   Relevance Score: {source.relevance_score:.2f}\")\n",
        "    else:\n",
        "        print(\"\\nNo evidence sources recorded by the agent.\")\n",
        "\n",
        "\n",
        "    # Final assessment summary\n",
        "    print(\"\\n\\n\" + \"=\"*100)\n",
        "    print(\"📊 **FINAL ASSESSMENT SUMMARY**\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    # You can combine insights from both fact_check_result and research_result here\n",
        "    print(f\"\\n**Fact-Checking Confidence Level:** {fact_check_result.get('final_confidence', 0):.2%}\")\n",
        "    print(f\"**Agent's Confidence Assessment:** {research_result.confidence_assessment.get('overall', 0):.2%}\")\n",
        "    print(f\"**Agent's Research Completeness:** {research_result.confidence_assessment.get('completeness', 0):.2%}\")\n",
        "    print(f\"**Agent's Source Quality Assessment:** {research_result.confidence_assessment.get('source_quality', 0):.2%}\")\n",
        "\n",
        "    # You could add a final summary generated by an LLM chain here if needed\n",
        "    # Example: final_summary = summary_chain.run(text=research_result.summary + json.dumps(fact_check_result))\n",
        "    # print(\"\\n**Overall Summary:**\\n\", final_summary)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"claim\": claim,\n",
        "        \"fact_check_result\": fact_check_result,\n",
        "        \"research_result\": research_result\n",
        "    }\n",
        "\n",
        "# @title Run Legal Research on Selected Claim\n",
        "\n",
        "# Select a claim to analyze\n",
        "selected_claim = claim_4  # Using the Indian Law Claim\n",
        "\n",
        "print(\"🏛️ INDIAN LEGAL RESEARCH SYSTEM\")\n",
        "print(\"=\"*100)\n",
        "print(f\"\\n🔍 Analyzing: {selected_claim}\\n\")\n",
        "\n",
        "# Execute the research\n",
        "results = execute_legal_research(selected_claim, legal_research_agent, fact_checker)\n",
        "\n",
        "# @title Additional Analysis Functions using LangChain (Optional)\n",
        "\n",
        "# These functions can be used independently after execute_legal_research\n",
        "# They are not part of the automated execute_legal_research flow above\n",
        "# unless explicitly called within it.\n",
        "\n",
        "# Example: Generate Legal Opinion based on results (requires a summary chain or direct LLM call)\n",
        "# def generate_legal_opinion(llm, claim, research_results):\n",
        "#     \"\"\"Generate a formal legal opinion using LangChain\"\"\"\n",
        "#     # This would need a prompt and potentially structured input from research_results\n",
        "#     pass\n",
        "\n",
        "# Example: Create Citation Formatter (if needed for final presentation)\n",
        "# def format_citations_properly(citations):\n",
        "#     \"\"\"Format legal citations according to Indian legal citation standards\"\"\"\n",
        "#     pass\n",
        "\n",
        "\n",
        "# @title Save Research Results (Optional)\n",
        "\n",
        "def save_research_results(results, filename=\"legal_research_report.json\"):\n",
        "    \"\"\"Save the research results to a JSON file\"\"\"\n",
        "\n",
        "    # Prepare data for JSON serialization - be mindful of complex objects\n",
        "    save_data = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"claim\": results[\"claim\"],\n",
        "        \"fact_checking_confidence\": results[\"fact_check_result\"].get(\"final_confidence\", 0),\n",
        "        \"fact_checking_steps_count\": len(results[\"fact_check_result\"].get(\"verification_steps\", [])),\n",
        "        \"agent_research_summary\": results[\"research_result\"].summary,\n",
        "        \"agent_extracted_citations\": results[\"research_result\"].citations[:10] if results[\"research_result\"].citations else [],\n",
        "        \"agent_evidence_sources_count\": len(results[\"research_result\"].evidence_chain),\n",
        "        \"agent_jurisdictional_notes\": results[\"research_result\"].jurisdictional_notes,\n",
        "        \"agent_confidence_assessment\": results[\"research_result\"].confidence_assessment,\n",
        "        # Potentially include summarized steps from fact_check_result if needed\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(save_data, f, indent=2)\n",
        "        print(f\"\\n✅ Research results summary saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error saving research results: {str(e)}\")\n",
        "\n",
        "\n",
        "# Save the results (optional)\n",
        "save_research_results(results)\n",
        "\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*100)\n",
        "print(\"✅ LEGAL RESEARCH AND FACT-CHECKING PROCESS COMPLETE\")\n",
        "print(\"=\"*100)\n",
        "print(\"\\nThe system has executed the legal research and fact-checking process.\")\n",
        "print(\"Review the output above for the detailed report, analysis, and confidence scores.\")"
      ],
      "metadata": {
        "id": "r_QX0KA7GzGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}